# Removendo Duplicatas

Nós ordenamos o arquivo de logs filtrados, conforme a data. Porém, ao criar esse arquivo e adicionar tanto linhas de logs de erro quanto de dados sensíveis, algumas linhas ficaram duplicadas, pois os dados sensíveis, às vezes, são mensagens de erro também.

No terminal, na pasta dos logs, podemos verificar isso ao executar o comando `cat myapp.backend.log.filtrado`. As duas últimas linhas desse arquivo são idênticas, com a mesma data, horário e mensagem:
```
    2024-09-03 12:00:00 ERROR: SENSITIVE_DATA: Database backup contains sensitive information.

    2024-09-03 12:00:00 ERROR: SENSITIVE_DATA: Database backup contains sensitive information.
```
Isso aconteceu devido à presença das palavras "SENSITIVE_DATA" e "ERROR" na mesma linha dos logs.

Nosso objetivo é remover essas duplicatas para obter um arquivo mais limpo, facilitando a análise.

## Removendo duplicatas com uniq

Para remover duplicatas no Linux, utilizamos o comando uniq:
```bash
uniq myapp-backend.log.filtrado
```
A saída do arquivo exibe as informações sem duplicatas. Notaremos que a última linha não está mais repetida, aparecendo apenas uma vez.

Para o uniq funcionar corretamente, é necessário que o arquivo esteja ordenado. Na aula passada, nós o ordenamos por data. Caso ele não estivesse ordenado, o comando não funcionaria corretamente.

O uniq procura duplicatas no arquivo e mantém apenas a primeira ocorrência de cada linha. Portanto, as linhas precisam estar consecutivas no arquivo para ele manter apenas uma de cada.

Além disso, é importante lembrar que o uniq não altera o arquivo diretamente. Ele faz as alterações apenas na saída do terminal. Se executarmos `cat myapp-backend.log.filtrado `novamente, ainda veremos as duplicatas.

Para salvar as informações sem duplicatas, podemos usar o operador de redirecionamento ">" seguido do nome do arquivo em que vamos guardar as informações:

```bash
uniq myapp-backend.log.filtrado > logs-sem-duplicatas
```
Assim, criamos um novo arquivo com os logs filtrados sem informações repetidas. Podemos executar o cat logs-sem-duplicatas para nos certificar.

## Incrementando o script de monitoramento

No final do arquivo `monitoramento-logs.sh`, após o sort, vamos inserir o comando uniq. Substituiremos myapp-backend.log.filtrado pelo nome da variável "$arquivo.filtrado". Após o operador de redirecionamento, criaremos um arquivo chamado ${arquivo}.unico, indicando que os logs já foram filtrados e não possuem duplicatas:

```bash
#!/bin/bash

LOG_DIR="../myapp/logs"

echo "Verificando logs no diretorio $LOG_DIR"

find $LOG_DIR -name "*.log" -print0 | while IFS= read -r -d '' arquivo; do
    grep "ERROR" "$arquivo" > "${arquivo}.filtrado"
    grep "SENSITIVE_DATA" "$arquivo" >> "${arquivo}.filtrado"

    sed -i 's/User password is .*/User password is REDACTED/g' "${arquivo}.filtrado"
    sed -i 's/User password reset request with token .*/User password reset request with token REDACTED/g' "${arquivo}.filtrado"
    sed -i 's/API key leaked: .*/API key leaked: REDACTED/g' "${arquivo}.filtrado"
    sed -i 's/User credit card last four digits: .*/User credit card last four digits: REDACTED/g' "${arquivo}.filtrado"
    sed -i 's/User session initiated with token: .*/User session initiated with token: REDACTED/g' "${arquivo}.filtrado"

    sort "${arquivo}.filtrado" -o "${arquivo}.filtrado"

    uniq "${arquivo}.filtrado" > "${arquivo}.unico" # <->
done
```
Após salvar as alterações e sair do Vim, vamos executar o script. Na pasta de logs, vamos listar os arquivos com ls. Agora, temos arquivos com a extensão .unico.

Ao executar cat myapp-backend.log.único, notaremos que os logs de erro e os logs dados sensíveis estão ordenados por data e sem duplicatas.

Na sequência, aprenderemos alguns comandos úteis para visualizar o conteúdo de arquivos no terminal.